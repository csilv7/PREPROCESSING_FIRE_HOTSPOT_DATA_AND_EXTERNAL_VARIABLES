# -*- coding: utf-8 -*-
"""code_data_fire.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pC3gSA2V-7hfmCpG65iHao8As5ENBGdy

# ‚úÖ Demanda de Consolida√ß√£o de Base de Dados

## üß† **Objetivo:** Gerar Conjunto de Dados Consolidado Final

### 1. Dados de Cicatrizes de Queimadas üî•
### 2. Dados N¬∫ de Focus de Inc√™ndio, N¬∫ de Interna√ß√µes e Emiss√£o de PM 2.5 üî•

Ser√° escrita uma sintaxe em Python, fazendo uso das seguintes bibliotecas:

* `os`
* `glob`
* `numpy`
* `pandas`

Tendo como finalidade gerar conjunto de dados que contenha informa√ß√µes como **N¬∫ de Hospitaliza√ß√µes**; **N¬∫ de Focos de Inc√™ndio** e; a **Emiss√£o de PM 2.5** *M√©dia*, *Mediana* e *M√°xima* de **2005** a **2024** por *Munic√≠pio*, *Ano* e *M√™s*.

---

* **Requerente:** CEL QOBM WAGNER
* **Analista:** VC Cau√£ Rodrigues

---

Para o Google Colaboratory:
```python
# Conex√£o com o Google Drive
from google.colab import drive
drive.mount("/content/drive")
```

# (1) Dados de Cicatrizes de Queimadas

```python
!pip install rioxarray

import os
import glob
import xarray as xr
import rioxarray
import pandas as pd
import numpy as np

# Definir o caminho da pasta onde est√£o os arquivos
path_to_files = "/content/drive/MyDrive/CBMPA/DEMANDAS INDIVIDUAIS/DADOS DE QUEIMADAS/DADOS TIF/"
file_pattern = "mapbiomas_fire_col4_br_annual_burned_*.tif"

# Listar e ordenar os arquivos para garantir a sequ√™ncia cronol√≥gica
files = sorted(glob.glob(os.path.join(path_to_files, file_pattern)))

# Extrair os anos dos nomes dos arquivos
years = [int(f.split('_')[-1].split('.')[0]) for f in files]

# Fun√ß√£o de leitura com CHUNKS (ativando o Dask)
def load_with_dask(file, year):
    # Definimos chunks de 1000x1000 pixels para n√£o sobrecarregar a RAM
    da = rioxarray.open_rasterio(file, chunks={'x': 1000, 'y': 1000})
    da = da.expand_dims(time=[pd.to_datetime(year, format='%Y')])
    return da

# Concatenar (isso ser√° instant√¢neo, pois ele n√£o ler√° os dados de fato agora)
data_cube = xr.concat([load_with_dask(f, y) for f, y in zip(files, years)], dim="time")

# Limpeza de dimens√µes desnecess√°rias
if 'band' in data_cube.dims:
    data_cube = data_cube.squeeze("band").drop_vars("band")

print("Cubo de dados mapeado com sucesso (Lazy Loading ativo)!")

data_cube
```

# (2) Dados N¬∫ de Focus de Inc√™ndio, N¬∫ de Interna√ß√µes e Emiss√£o de PM 2.5
"""

import os
import glob
import pandas as pd

# Mapeamento dos meses para categoriza√ß√£o
month_mapping = {
    1: "Janeiro", 2: "Fevereiro", 3: "Mar√ßo", 4: "Abril", 5: "Maio", 6: "Junho",
    7: "Julho", 8: "Agosto", 9: "Setembro", 10: "Outubro", 11: "Novembro", 12: "Dezembro"
}

"""## (2.1) Dados de Focos de Inc√™ndio - 1995 √† 2024

### (A) Leitura Automatizda
"""

# Definir o caminho da pasta onde est√£o os arquivos
path_to_files = "C:/Users/breno/Documents/CBMPA/DEMANDAS INDIVIDUAIS/CEL WAGNER/001 - Arquivos Excel Focos 1995-2024/"

# Listar os arquivos .CSV
list_of_files = glob.glob(os.path.join(path_to_files, "*.csv"))

# Ordenar
list_of_files.sort()

# Leitura automatizada dos dados
list_of_dfs = [
    pd.read_csv(
        file_path,
        dtype={"Municipio": "string"},
        parse_dates=["DataHora"]
    ) for file_path in list_of_files
]

# Concatena√ß√£o dos dados
df_concat = pd.concat(list_of_dfs, ignore_index=True)

# Info do Data Frame
display(df_concat.info())

# Exibir as primeiras linhas do DataFrame
display(df_concat.head())
print("\n")
# Exibir as √∫ltimas linhas do DataFrame
display(df_concat.tail())

"""* **Nota:** os dados foram checados manualmente antes de virem para o Python. Por isso foi utilizada essa abordagem de leitura automatiada.

### (B) Tratamento de Dados
"""

# Convertendo a coluna "DataHora" para o formato datetime
df_concat["DATA"] = pd.to_datetime(df_concat["DataHora"], errors="coerce")

# Extraindo a hora
df_concat["HORA"] = df_concat["DATA"].dt.time

# Excluir o atributo "DataHora"
df_concat.drop(columns=["DataHora"], inplace=True)

# Extrair o atributo "ANO" da atributo "DATA"
df_concat["ANO"] = df_concat["DATA"].dt.year

# Executar filtro para selecionar os dados a partir do ano de 2005
df_concat = df_concat[df_concat["ANO"] >= 2005]

# Extraindo o atributo "M√äS" da atributo "DATA"
df_concat["M√äS"] = df_concat["DATA"].dt.month

# Extraindo o atributo "DIA" da atributo "DATA"
df_concat["DIA"] = df_concat["DATA"].dt.day

# Aplicando o mapeamento para categorizar os meses (equivalente a Factor em R)
df_concat["M√äS"] = pd.Categorical(
    df_concat["M√äS"].map(month_mapping),
    categories=list(month_mapping.values()),
    ordered=True
)

# Ordenar Data Frame por "DATA"
df_concat.sort_values(by="DATA", inplace=True)

# Ajuste do nome dos atrubutos para caixa alta
df_concat.columns = df_concat.columns.str.upper()

# Resetar o √≠ndice
df_concat.reset_index(drop=True, inplace=True)

# Exibir informa√ß√µes do DataFrame ap√≥s as transforma√ß√µes
display(df_concat.info())

# Exibir as primeiras linhas do DataFrame ap√≥s as transforma√ß√µes
display(df_concat.head())
print("\n")
# Exibir as √∫ltimas linhas do DataFrame ap√≥s as transforma√ß√µes
display(df_concat.tail())

"""### (C) Salvar Dados ap√≥s Pr√© processamento"""

# Salvar o Conjunto de Dados em Excel
df_concat.to_excel("C:/Users/breno/Documents/CBMPA/DEMANDAS INDIVIDUAIS/CEL WAGNER/data_focus_fire_2005_2024.xlsx", index=False)

# Salvar o Conjunto de Dados em CSV
df_concat.to_csv("C:/Users/breno/Documents/CBMPA/DEMANDAS INDIVIDUAIS/CEL WAGNER/data_focus_fire_2005_2024.csv", index=False)

"""## (2.2) Agrupamento com os Dados de:
* ### N¬∫ de Interna√ß√µes Hospitalares
* ### Emiss√£o de *PM 2.5* (https://zenodo.org/records/16374139?utm_source=chatgpt.com)
"""

from geobr import read_municipality
import re
import unicodedata

# Gerar um Data Frame simplificado p/ agrupamento
df_simplified = pd.merge(
    left=df_concat.groupby(by=["MUNICIPIO", "ANO", "M√äS"], as_index=False)["DIA"].count().rename(columns={"DIA": "CONTAGEM_FOCOS"}),
    right=df_concat.groupby(by=["MUNICIPIO", "ANO", "M√äS"], as_index=False)["DIASEMCHUVA"].sum().rename(columns={"DIASEMCHUVA": "CONTAGEM_DIAS_SEM_CHUVA"}),
    on=["MUNICIPIO", "ANO", "M√äS"], # Correspond√™ncia dos atributos para o merge
    how="outer"                     # M√©todo de jun√ß√£o (inner, left, right, outer)
)

# Para facilitar a jun√ß√£o com os demais dados
df_simplified.columns = df_simplified.columns.str.lower()

"""### (A) Tratamento para os Dados Hospitalares"""

# Definir o caminho da pasta dos arquivos de dados de Hospitaliza√ß√µes
path_to_files_of_hosp = "C:/Users/breno/Documents/CBMPA/DEMANDAS INDIVIDUAIS/CEL WAGNER/DADOS DE HOPITALIZA√á√ïES - 1998 A 2024/"

# Listar os arquivos .XLSX
list_of_files_of_hosp = glob.glob(os.path.join(path_to_files_of_hosp, "*.xlsx"))

# Ordenar
list_of_files_of_hosp.sort()

# Leitura dos dados
dfHosp_98_07, dfHosp_08_24 = pd.read_excel(list_of_files_of_hosp[0]), pd.read_excel(list_of_files_of_hosp[1])

# Informa√ß√µes dos dados de 1998 a 2007
display(dfHosp_98_07.info())

# Informa√ß√µes dos dados de 2008 a 2024
display(dfHosp_08_24.info())

# Fun√ß√£o de Pivotagem ajustada para os dados de hospitaliza√ß√µes
def pivot_hospitalizations(df, id_vars):
    """
    Fun√ß√£o (n√£o gen√©rica) para pivotar os dados de hospitaliza√ß√µes, transformando as colunas de anos em uma √∫nica coluna de datas e os valores correspondentes em uma coluna de contagem de hospitaliza√ß√µes.
    Par√¢metros:
    - df: DataFrame contendo os dados de hospitaliza√ß√µes.
    - id_vars: Lista de colunas que devem permanecer como identificadores (ex: "Munic√≠pio").
    Retorna:
    - df_pivoted: DataFrame pivotado com as colunas de anos transformadas em uma coluna de datas e os valores correspondentes em uma coluna de contagem de hospitaliza√ß√µes.
    """
    # Atributos que ser√£o pivotados
    value_vars = df.columns.difference(id_vars).tolist()

    # Verificar se os atributos existem no DataFrame
    if not all(var in df.columns for var in id_vars + value_vars):
        raise ValueError("Alguns dos atributos especificados n√£o existem no DataFrame.")

    # Realizar a pivotagem
    df_pivoted = df.melt(
        id_vars=id_vars,
        value_vars=value_vars,
        var_name="date",
        value_name="n_hospitalizations"
    )

    # Ajuste no atributo "Munic√≠pio"
    df_pivoted["Munic√≠pio"] = df_pivoted["Munic√≠pio"].str.split().str[1:].str.join(" ")

    # Ajuste no atributo "n_hospitalizations" para garantir que seja num√©rico
    df_pivoted["n_hospitalizations"] = pd.to_numeric(
        arg=df_pivoted["n_hospitalizations"],
        errors="coerce"
    ).fillna(0).astype(int)

    # Coverter o atributo "date" para o formato datetime
    df_pivoted["date"] = pd.to_datetime(
        arg=df_pivoted["date"],
        errors="coerce"
    )

    # Extra√ß√£o dos atributos "ano", "m√™s" e "trimestre" a partir do atributo "date"
    df_pivoted["ano"] = df_pivoted["date"].dt.year
    df_pivoted["m√™s"] = df_pivoted["date"].dt.month
    df_pivoted["trimestre"] = df_pivoted["date"].dt.quarter

    # Aplicando o mapeamento para categorizar os meses (equivalente a Factor em R)
    df_pivoted["m√™s"] = pd.Categorical(
        df_pivoted["m√™s"].map(month_mapping),
        categories=list(month_mapping.values()),
        ordered=True
    )

    # Criar atributo "per√≠odo" (Chuvoso/N√£o Chuvoso) com base no atributo "m√™s"
    df_pivoted["per√≠odo"] = df_pivoted["m√™s"].apply(
        lambda x: "Chuvoso" if x in ["Dezembro", "Janeiro", "Fevereiro", "Mar√ßo", "Abril", "Maio"] else "N√£o Chuvoso"
    )

    # Retornar o DataFrame pivotado
    return df_pivoted

# Gerer um Data Frama concatenado
df_concat_hosp = pd.concat(
    objs=[
    pivot_hospitalizations(dfHosp_98_07, id_vars=["Munic√≠pio"]),
    pivot_hospitalizations(dfHosp_08_24, id_vars=["Munic√≠pio"])
    ],
    ignore_index=True
)

# Salvar o Conjunto de Dados em Excel
df_concat_hosp.to_excel("C:/Users/breno/Documents/CBMPA/DEMANDAS INDIVIDUAIS/CEL WAGNER/data_hospitalizations_1998_2024.xlsx", index=False)

# Salvar o Conjunto de Dados em CSV
df_concat_hosp.to_csv("C:/Users/breno/Documents/CBMPA/DEMANDAS INDIVIDUAIS/CEL WAGNER/data_hospitalizations_1998_2024.csv", index=False)

"""### (B) Tratamento para os Dados de Emiss√£o de *PM 2.5*"""

# Definir caminho para os dados de PM 2.5
path_to_pm25 = "C:\\Users\\breno\\Documents\\CBMPA\\DEMANDAS INDIVIDUAIS\\CEL WAGNER\\GEO\\pm25_max_mean.csv"

# Leitura dos dados de PM 2.5
df_pm25 = pd.read_csv(path_to_pm25, parse_dates=["date"])

# Informa√ß√µes do DataFrame de PM 2.5
display(df_pm25.info())

# Exibir as primeiras linhas do DataFrame
display(df_pm25.head())
print("\n")
# Exibir as √∫ltimas linhas do DataFrame
display(df_pm25.tail())

# Executar filtro para selecionar apenas dados do Estado do Par√°
df_pm25 = df_pm25[df_pm25["code_muni"].astype(str).str.startswith("15")]

# Ajuste no atributo "code_muni" para manter apenas os 6 d√≠gitos
df_pm25["code_muni"] = df_pm25["code_muni"].astype(str).str[:7].astype(int)

# Resetar o √≠ndice do DataFrame de PM 2.5
df_pm25.reset_index(drop=True, inplace=True)

# Informa√ß√µes do DataFrame de PM 2.5 ap√≥s o filtro
display(df_pm25.info())

# Exibir as primeiras linhas do DataFrame
display(df_pm25.head())
print("\n")
# Exibir as √∫ltimas linhas do DataFrame
display(df_pm25.tail())

# Leitura dos dados dos munic√≠pios utilizando a biblioteca geobr
df_muni = read_municipality(code_muni=15, year=2024)[["code_muni", "name_muni"]]

# Ajuste do tipo de dado do atributo "code_muni" para inteiro
df_muni["code_muni"] = df_muni["code_muni"].astype(int)

# Integrar os dados de PM 2.5 com os nomes dos munic√≠pios
df_pm25 = pd.merge(
    left=df_pm25,
    right=df_muni,
    on="code_muni",
    how="left"
)

# Criar atributo de ano e m√™s a partir do atributo "date"
df_pm25["ano"] = df_pm25["date"].dt.year
df_pm25["m√™s"] = df_pm25["date"].dt.month

# Executar filtro para selecionar os dados a partir do ano de 2005
df_pm25 = df_pm25[df_pm25["ano"] >= 2005]

# Aplicando o mapeamento para categorizar os meses (equivalente a Factor em R)
df_pm25["m√™s"] = pd.Categorical(
    df_pm25["m√™s"].map(month_mapping),
    categories=list(month_mapping.values()),
    ordered=True
)

# Ordenar Data Frame por "date"
df_pm25.sort_values(by="date", inplace=True)

# Informa√ß√µes do Data Frame integrado
display(df_pm25.info())

# Agrupar dados por munic√≠pio, ano e m√™s, calculando mpetricas para PM 2.5 (atributo "value")
df_pm25_grouped = df_pm25.groupby(
    by=["code_muni", "name_muni", "ano", "m√™s"],
    as_index=False
).agg(
    {
        "value": ["mean", "median", "max"],
        "ano": "first",
        "m√™s": "first"
    }
)

# Renomear os atributos agregados de PM 2.5 para facilitar a interpreta√ß√£o
df_pm25_grouped.columns = [
    col[0] if col[1] in ["", "first"] else "_".join(col).strip() for col in df_pm25_grouped.columns
]

# Exibir as primeiras linhas do DataFrame
display(df_pm25_grouped.head())
print("\n")
# Exibir as √∫ltimas linhas do DataFrame
display(df_pm25_grouped.tail())

# Salvar o Conjunto de Dados em Excel
df_pm25_grouped.to_excel("C:/Users/breno/Documents/CBMPA/DEMANDAS INDIVIDUAIS/CEL WAGNER/data_pm25_2005_2024.xlsx", index=False)

# Salvar o Conjunto de Dados em CSV
df_pm25_grouped.to_csv("C:/Users/breno/Documents/CBMPA/DEMANDAS INDIVIDUAIS/CEL WAGNER/data_pm25_2005_2024.csv", index=False)

"""### (C) Jun√ß√£o dos Dados"""

def str_squish_python(text):
    # Remove espa√ßos em branco do in√≠cio e fim, depois substitui m√∫ltiplos espa√ßos por um √∫nico
    return re.sub(r'\s+', ' ', text).strip()

def remove_accents_python(text):
    nfkd_form = unicodedata.normalize('NFKD', text)
    return ''.join([c for c in nfkd_form if not unicodedata.combining(c)])

# Ajuste para garantir a correspond√™ncia dos nomes dos munic√≠pios entre os DataFrames: de focos de inc√™ndio e hospitaliza√ß√µes
df_simplified["name_muni"] = df_simplified["municipio"].apply(
    lambda x: remove_accents_python(str_squish_python(x)).upper() if pd.notnull(x) else x
)
# de hospitaliza√ß√µes
df_concat_hosp["name_muni"] = df_concat_hosp["Munic√≠pio"].apply(
    lambda x: remove_accents_python(str_squish_python(x)).upper() if pd.notnull(x) else x
)
# e de PM 2.5
df_pm25_grouped["name_muni"] = df_pm25_grouped["name_muni"].apply(
    lambda x: remove_accents_python(str_squish_python(x)).upper() if pd.notnull(x) else x
)

# Excluir atributo "MUNICIPIO" do DataFrame de focos de inc√™ndio e o atributo "Munic√≠pio" do DataFrame de hospitaliza√ß√µes para evitar redund√¢ncia ap√≥s a cria√ß√£o do atributo "name_muni"
df_simplified.drop(columns=["municipio"], inplace=True)
df_concat_hosp.drop(columns=["Munic√≠pio", "date"], inplace=True)

# Executar jun√ß√£o entre os DataFrames de focos de inc√™ndio e hospitaliza√ß√µes com base no atributo "name_muni"
df_joined = pd.merge(
    left=df_simplified,
    right=df_concat_hosp,
    on=["name_muni", "ano", "m√™s"],
    how="left"
).merge(
    right=df_pm25_grouped,
    on=["name_muni", "ano", "m√™s"],
    how="left"
)

# Visualizar o Data Frame resultante da jun√ß√£o
display(df_joined)

# Salvar o Conjunto de Dados em Excel
df_joined.to_excel("C:/Users/breno/Documents/CBMPA/DEMANDAS INDIVIDUAIS/CEL WAGNER/data_complete.xlsx", index=False)

# Salvar o Conjunto de Dados em CSV
df_joined.to_csv("C:/Users/breno/Documents/CBMPA/DEMANDAS INDIVIDUAIS/CEL WAGNER/data_complete.csv", index=False)